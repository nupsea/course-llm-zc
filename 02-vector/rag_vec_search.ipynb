{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c79ef896",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sethurama/.local/share/virtualenvs/course-llm-zc-Yowha_Zo/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "from qdrant_client import QdrantClient, models\n",
    "from fastembed import  TextEmbedding\n",
    "\n",
    "from openai import OpenAI\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23428e19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"The purpose of this document is to capture frequently asked technical questions\\nThe exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\\nSubscribe to course public Google Calendar (it works from Desktop only).\\nRegister before the course starts using this link.\\nJoin the course Telegram channel with announcements.\\nDon’t forget to register in DataTalks.Club's Slack and join the channel.\",\n",
       " 'section': 'General course-related questions',\n",
       " 'question': 'Course - When will the course start?',\n",
       " 'course': 'data-engineering-zoomcamp'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "docs_url = 'https://github.com/alexeygrigorev/llm-rag-workshop/raw/main/notebooks/documents.json'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()\n",
    "\n",
    "docs = []\n",
    "for c in documents_raw:\n",
    "    for d in c['documents']:\n",
    "        d['course'] = c['course']\n",
    "        docs.append(d)\n",
    "\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b6891a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "qd_client = QdrantClient(\"http://localhost:6333\") \n",
    "\n",
    "EMBEDDING_DIMENSIONS = 512\n",
    "model_handle = \"jinaai/jina-embeddings-v2-small-en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25c26a75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection_name = \"zoomcamp-faq\"\n",
    "qd_client.delete_collection(collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01e11ce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "qd_client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=models.VectorParams(\n",
    "        size=EMBEDDING_DIMENSIONS,\n",
    "        distance=models.Distance.COSINE\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edbf3052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qd_client.delete_collection(collection_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ddd4d92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'GitHub - DataTalksClub data-engineering-zoomcamp#prerequisites',\n",
       " 'section': 'General course-related questions',\n",
       " 'question': 'Course - What are the prerequisites for this course?',\n",
       " 'course': 'data-engineering-zoomcamp'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81174739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=1, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qd_client.create_payload_index(\n",
    "    collection_name=collection_name,\n",
    "    field_name=\"course\",\n",
    "    field_schema=\"keyword\" # exact match on string metadata field\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54b4460b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=2, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = []\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    \n",
    "    q_a = doc['question'] + ' ' + doc['text']  # Concatenate question and text for embedding\n",
    "    vector=models.Document(text=q_a, model=model_handle)\n",
    "\n",
    "    point = models.PointStruct(\n",
    "        id=i,\n",
    "        vector=vector,\n",
    "        payload=doc\n",
    "    )\n",
    "    points.append(point)\n",
    "\n",
    "qd_client.upsert(\n",
    "    collection_name=collection_name,\n",
    "    points=points\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43d4f109",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def vector_search(question, course=\"data-engineering-zoomcamp\", limit=5):\n",
    "    print(f\"Using Vector Search with filter: {course}. Results limit: {limit}\")\n",
    "    \n",
    "    q_points = qd_client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=models.Document(\n",
    "            text=question,\n",
    "            model=model_handle\n",
    "        ),\n",
    "        query_filter=models.Filter(\n",
    "            must=[\n",
    "                models.FieldCondition(\n",
    "                    key=\"course\",\n",
    "                    match=models.MatchValue(value=course)\n",
    "                )\n",
    "            ]\n",
    "        ),\n",
    "        limit=limit,\n",
    "        with_payload=True\n",
    "    )\n",
    "\n",
    "    results = []\n",
    "    for point in q_points.points:\n",
    "        results.append(point.payload)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d42c7a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Vector Search with filter: data-engineering-zoomcamp. Results limit: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'text': 'Ans: No, it is not.',\n",
       "  'section': 'Workshop 2 - RisingWave',\n",
       "  'question': 'Setup - Qn: Is kafka install required for the RisingWave workshop? [source]',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'If you have this error, it most likely that your kafka broker docker container is not working.\\nUse docker ps to confirm\\nThen in the docker compose yaml file folder, run docker compose up -d to start all the instances.',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'kafka.errors.NoBrokersAvailable: NoBrokersAvailable',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'According to https://github.com/dpkp/kafka-python/\\n“DUE TO ISSUES WITH RELEASES, IT IS SUGGESTED TO USE https://github.com/wbarnha/kafka-python-ng FOR THE TIME BEING”\\nUse pip install kafka-python-ng instead',\n",
       "  'section': 'Project',\n",
       "  'question': 'How to fix the error \"ModuleNotFoundError: No module named \\'kafka.vendor.six.moves\\'\"?',\n",
       "  'course': 'data-engineering-zoomcamp'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_search(\"How to install Kafka?\", course=\"data-engineering-zoomcamp\", limit=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "affe76cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM API client\n",
    "\n",
    "%load_ext dotenv\n",
    "%dotenv /Users/sethurama/DEV/LM/course-llm-zc/.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd3a77c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_client = OpenAI()\n",
    "\n",
    "def build_prompt(q_question, search_results):\n",
    "    prompt_template = \"\"\"\n",
    "    You're a course teaching assistant. Answer the QUESTION based on the CONTEXT.\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "If the CONTEXT doesn't contain the answer, output NONE\n",
    "\n",
    "\n",
    "QUESTION: {question} \n",
    "\n",
    "CONTEXT: {context}\n",
    "\"\"\".strip()\n",
    "\n",
    "    context = \"\"\n",
    "\n",
    "    for doc in search_results:\n",
    "        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer:  {doc['text']}\\n\\n\"\n",
    "    \n",
    "    prompt = prompt_template.format(question=q_question, context=context).strip()\n",
    "    return prompt\n",
    "\n",
    "# Query the LLM with the modified prompt\n",
    "def query_llm(mod_prompt):\n",
    "    response = llm_client.chat.completions.create(\n",
    "        model = 'gpt-4o-mini',\n",
    "        messages = [{\"role\": \"user\", \"content\": mod_prompt}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb2abe3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query):\n",
    "    search_results = vector_search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer = query_llm(prompt)\n",
    "    return answer\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d1fb6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Vector Search with filter: data-engineering-zoomcamp. Results limit: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'To run Kafka in the terminal for Java, navigate to the project directory and execute the following command:\\n\\n```\\njava -cp build/libs/<jar_name>-1.0-SNAPSHOT.jar:out src/main/java/org/example/JsonProducer.java\\n```\\n\\nMake sure to replace `<jar_name>` with the actual name of your jar file.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag(\"How do I run kafka?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7f4e916",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hybrid Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56c930c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qdrant_client import models\n",
    "\n",
    "# Create the collection with spcified sparse vector parameters\n",
    "qd_client.create_collection(\n",
    "    collection_name=\"zoomcamp-sparse\",\n",
    "    sparse_vectors_config={\n",
    "        \"bm25\": models.SparseVectorParams(\n",
    "            modifier=models.Modifier.IDF\n",
    "        )\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1c000fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 18 files: 100%|██████████| 18/18 [00:00<00:00, 21.85it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=0, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "# Send the points to the collection\n",
    "qd_client.upsert(\n",
    "    collection_name=\"zoomcamp-sparse\",\n",
    "    points=[\n",
    "        models.PointStruct(\n",
    "            id=uuid.uuid4().hex,\n",
    "            vector={\n",
    "                \"bm25\": models.Document(\n",
    "                    text=doc[\"text\"],\n",
    "                    model=\"Qdrant/bm25\",\n",
    "                ),\n",
    "            },\n",
    "            payload={\n",
    "                \"text\": doc[\"text\"],\n",
    "                \"section\": doc[\"section\"],\n",
    "                \"course\": doc[\"course\"],\n",
    "            }\n",
    "        )\n",
    "        for course in documents_raw\n",
    "        for doc in course[\"documents\"]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e6d7992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running sparse vector search with BM25\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10fceb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_sparse(query: str, limit: int = 1) -> list[models.ScoredPoint]:\n",
    "    results = qd_client.query_points(\n",
    "        collection_name=\"zoomcamp-sparse\",\n",
    "        query=models.Document(\n",
    "            text=query,\n",
    "            model=\"Qdrant/bm25\"\n",
    "        ),\n",
    "        using=\"bm25\",\n",
    "        limit=limit,\n",
    "        with_payload=True\n",
    "    )\n",
    "\n",
    "    return results.points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7a06076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ScoredPoint(id='818cceb7-7529-4850-a5f8-f416e6125012', version=0, score=11.458191, payload={'text': \"✅SOLUTION: pip install confluent-kafka[avro].\\nFor some reason, Conda also doesn't include this when installing confluent-kafka via pip.\\nMore sources on Anaconda and confluent-kafka issues:\\nhttps://github.com/confluentinc/confluent-kafka-python/issues/590\\nhttps://github.com/confluentinc/confluent-kafka-python/issues/1221\\nhttps://stackoverflow.com/questions/69085157/cannot-import-producer-from-confluent-kafka\", 'section': 'Module 6: streaming with kafka', 'course': 'data-engineering-zoomcamp'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='7a05efa6-028f-431a-a9a3-f25bcd97a7ec', version=0, score=10.810294, payload={'text': 'If you get an error while running the command python3 stream.py worker\\nRun pip uninstall kafka-python\\nThen run pip install kafka-python==1.4.6\\nWhat is the use of  Redpanda ?\\nRedpanda: Redpanda is built on top of the Raft consensus algorithm and is designed as a high-performance, low-latency alternative to Kafka. It uses a log-centric architecture similar to Kafka but with different underlying principles.\\nRedpanda is a powerful, yet simple, and cost-efficient streaming data platform that is compatible with Kafka® APIs while eliminating Kafka complexity.', 'section': 'Module 6: streaming with kafka', 'course': 'data-engineering-zoomcamp'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='4486ab43-713e-4f48-b1f2-f48b5f2dff21', version=0, score=10.748172, payload={'text': 'confluent-kafka: `pip install confluent-kafka` or `conda install conda-forge::python-confluent-kafka`\\nfastavro: pip install fastavro\\nAbhirup Ghosh\\nCan install Faust Library for Module 6 Python Version due to dependency conflicts?\\nThe Faust repository and library is no longer maintained - https://github.com/robinhood/faust\\nIf you do not know Java, you now have the option to follow the Python Videos 6.13 & 6.14 here https://www.youtube.com/watch?v=BgAlVknDFlQ&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=80  and follow the RedPanda Python version here https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/06-streaming/python/redpanda_example - NOTE: I highly recommend watching the Java videos to understand the concept of streaming but you can skip the coding parts - all will become clear when you get to the Python videos and RedPanda files.', 'section': 'Module 6: streaming with kafka', 'course': 'data-engineering-zoomcamp'}, vector=None, shard_key=None, order_value=None)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = search_sparse(\"How to install Kafka?\", limit=3)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df51fb89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = search_sparse(\"Qdrant\", limit=3)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e1f195b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can use round() function or f-strings\n",
      "round(number, 4)  - this will round number up to 4 decimal places\n",
      "print(f'Average mark for the Homework is {avg:.3f}') - using F string\n",
      "Also there is pandas.Series. round idf you need to round values in the whole Series\n",
      "Please check the documentation\n",
      "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.round.html#pandas.Series.round\n",
      "Added by Olga Rudakova\n"
     ]
    }
   ],
   "source": [
    "results = search_sparse(\"pandas\", limit=3)\n",
    "print(results[0].payload['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81a2268f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0392046"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scores returned by BM25 are not calculated with cosine similarity, but with BM25 formula. They are not bounded to a specific range, but are virtually unbounded.\n",
    "results[0].score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "004a1df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Natural language like queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99717561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"text\": \"Even though the upload works using aws cli and boto3 in Jupyter notebook.\\nSolution set the AWS_PROFILE environment variable (the default profile is called default)\",\n",
      "  \"section\": \"Module 4: Deployment\",\n",
      "  \"question\": \"Uploading to s3 fails with An error occurred (InvalidAccessKeyId) when calling the PutObject operation: The AWS Access Key Id you provided does not exist in our records.\\\"\",\n",
      "  \"course\": \"mlops-zoomcamp\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "\n",
    "random.seed(202506)\n",
    "\n",
    "course = random.choice(documents_raw)\n",
    "course_piece = random.choice(course[\"documents\"])\n",
    "print(json.dumps(course_piece, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bef11723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The trial dbt account provides access to dbt API. Job will still be needed to be added manually. Airflow will run the job using a python operator calling the API. You will need to provide api key, job id, etc. (be careful not committing it to Github).\n",
      "Detailed explanation here: https://docs.getdbt.com/blog/dbt-airflow-spiritual-alignment\n",
      "Source code example here: https://github.com/sungchun12/airflow-toolkit/blob/95d40ac76122de337e1b1cdc8eed35ba1c3051ed/dags/examples/dbt_cloud_example.py\n"
     ]
    }
   ],
   "source": [
    "results = search_sparse(course_piece[\"question\"])\n",
    "print(results[0].payload[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d634f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Qdrant Universal Query API - prefetching\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "541254f8",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "ascii\n",
    "┌─────────────┐           ┌─────────────┐\n",
    "│             │           │             │\n",
    "│  Retrieval  │ ────────► │  Reranking  │\n",
    "│             │           │             │\n",
    "└─────────────┘           └─────────────┘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aff302ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the collection with both vector types\n",
    "qd_client.create_collection(\n",
    "    collection_name=\"zoomcamp-sparse-and-dense\",\n",
    "    vectors_config={\n",
    "        # Named dense vector for jinaai/jina-embeddings-v2-small-en\n",
    "        \"jina-small\": models.VectorParams(\n",
    "            size=512,\n",
    "            distance=models.Distance.COSINE,\n",
    "        ),\n",
    "    },\n",
    "    sparse_vectors_config={\n",
    "        \"bm25\": models.SparseVectorParams(\n",
    "            modifier=models.Modifier.IDF,\n",
    "        )\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a769a686",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 5 files: 100%|██████████| 5/5 [00:02<00:00,  2.08it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=0, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qd_client.upsert(\n",
    "    collection_name=\"zoomcamp-sparse-and-dense\",\n",
    "    points=[\n",
    "        models.PointStruct(\n",
    "            id=uuid.uuid4().hex,\n",
    "            vector={\n",
    "                \"jina-small\": models.Document(\n",
    "                    text=doc[\"text\"],\n",
    "                    model=\"jinaai/jina-embeddings-v2-small-en\",\n",
    "                ),\n",
    "                \"bm25\": models.Document(\n",
    "                    text=doc[\"text\"], \n",
    "                    model=\"Qdrant/bm25\",\n",
    "                ),\n",
    "            },\n",
    "            payload={\n",
    "                \"text\": doc[\"text\"],\n",
    "                \"section\": doc[\"section\"],\n",
    "                \"course\": course[\"course\"],\n",
    "            }\n",
    "        )\n",
    "        for course in documents_raw\n",
    "        for doc in course[\"documents\"]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e0c11c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_stage_search(query: str, limit: int = 1) -> list[models.ScoredPoint]:\n",
    "    results = qd_client.query_points(\n",
    "        collection_name=\"zoomcamp-sparse-and-dense\",\n",
    "        prefetch=[\n",
    "            models.Prefetch(\n",
    "                query=models.Document(\n",
    "                    text=query,\n",
    "                    model=\"jinaai/jina-embeddings-v2-small-en\",\n",
    "                ),\n",
    "                using=\"jina-small\",\n",
    "                # Prefetch ten times more results, then\n",
    "                # expected to return, so we can really rerank\n",
    "                limit=(10 * limit),\n",
    "            ),\n",
    "        ],\n",
    "        query=models.Document(\n",
    "            text=query,\n",
    "            model=\"Qdrant/bm25\", \n",
    "        ),\n",
    "        using=\"bm25\",\n",
    "        limit=limit,\n",
    "        with_payload=True,\n",
    "    )\n",
    "\n",
    "    return results.points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bb1ee27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"text\": \"Even though the upload works using aws cli and boto3 in Jupyter notebook.\\nSolution set the AWS_PROFILE environment variable (the default profile is called default)\",\n",
      "  \"section\": \"Module 4: Deployment\",\n",
      "  \"question\": \"Uploading to s3 fails with An error occurred (InvalidAccessKeyId) when calling the PutObject operation: The AWS Access Key Id you provided does not exist in our records.\\\"\",\n",
      "  \"course\": \"mlops-zoomcamp\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(course_piece, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "053e5a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem description. How can we connect s3 bucket to MLFLOW?\n",
      "Solution: Use boto3 and AWS CLI to store access keys. The access keys are what will be used by boto3 (AWS' Python API tool) to connect with the AWS servers. If there are no Access Keys how can they make sure that they have the right to access this Bucket? Maybe you're a malicious actor (Hacker for ex). The keys must be present for boto3 to talk to the AWS servers and they will provide access to the Bucket if you possess the right permissions. You can always set the Bucket as public so anyone can access it, now you don't need access keys because AWS won't care.\n",
      "Read more here: https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html\n",
      "Added by Akshit Miglani\n"
     ]
    }
   ],
   "source": [
    "results = multi_stage_search(course_piece[\"question\"])\n",
    "print(results[0].payload[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b00c05ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Building Hybrid Search. \n",
    "\n",
    "def rrf_search(query: str, limit: int = 1) -> list[models.ScoredPoint]:\n",
    "    results = qd_client.query_points(\n",
    "        collection_name=\"zoomcamp-sparse-and-dense\",\n",
    "        prefetch=[\n",
    "            models.Prefetch(\n",
    "                query=models.Document(\n",
    "                    text=query,\n",
    "                    model=\"jinaai/jina-embeddings-v2-small-en\",\n",
    "                ),\n",
    "                using=\"jina-small\",\n",
    "                limit=(5 * limit),\n",
    "            ),\n",
    "            models.Prefetch(\n",
    "                query=models.Document(\n",
    "                    text=query,\n",
    "                    model=\"Qdrant/bm25\",\n",
    "                ),\n",
    "                using=\"bm25\",\n",
    "                limit=(5 * limit),\n",
    "            ),\n",
    "        ],\n",
    "        # Fusion query enables fusion on the prefetched results\n",
    "        query=models.FusionQuery(fusion=models.Fusion.RRF),\n",
    "        with_payload=True,\n",
    "    )\n",
    "\n",
    "    return results.points\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4efe9e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"text\": \"Even though the upload works using aws cli and boto3 in Jupyter notebook.\\nSolution set the AWS_PROFILE environment variable (the default profile is called default)\",\n",
      "  \"section\": \"Module 4: Deployment\",\n",
      "  \"question\": \"Uploading to s3 fails with An error occurred (InvalidAccessKeyId) when calling the PutObject operation: The AWS Access Key Id you provided does not exist in our records.\\\"\",\n",
      "  \"course\": \"mlops-zoomcamp\"\n",
      "}\n",
      "The trial dbt account provides access to dbt API. Job will still be needed to be added manually. Airflow will run the job using a python operator calling the API. You will need to provide api key, job id, etc. (be careful not committing it to Github).\n",
      "Detailed explanation here: https://docs.getdbt.com/blog/dbt-airflow-spiritual-alignment\n",
      "Source code example here: https://github.com/sungchun12/airflow-toolkit/blob/95d40ac76122de337e1b1cdc8eed35ba1c3051ed/dags/examples/dbt_cloud_example.py\n"
     ]
    }
   ],
   "source": [
    "results = rrf_search(course_piece[\"question\"])\n",
    "print(json.dumps(course_piece, indent=2))\n",
    "print(results[0].payload[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5727e87c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course-llm-zc-Yowha_Zo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
